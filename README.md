# Visualizaci√≥n de Atenci√≥n con PyTorch y Matplotlib

Este proyecto explora los conceptos de **hard attention**, **soft attention** y **self-attention** usando vectores simples y operaciones matriciales en PyTorch, con visualizaci√≥n en 2D mediante Matplotlib.

## Contenido

- `attention_visualization.py`: C√≥digo principal que:
  - Define un conjunto de vectores `X`.
  - Implementa hard attention (selecci√≥n uno a uno de vectores).
  - Implementa soft attention (distribuci√≥n de atenci√≥n sobre todos los vectores).
  - Implementa self-attention (atenci√≥n basada en similitud entre vectores).
  - Genera gr√°ficos para visualizar c√≥mo se aplican las atenciones.

## Conceptos aprendidos

1. **Hard Attention**
   - Solo un vector es atendido, los dem√°s se ignoran.
   - Se representa multiplicando un vector one-hot `a` por la matriz de vectores `X`.

2. **Soft Attention**
   - La atenci√≥n se distribuye entre todos los vectores.
   - Cada vector de salida es la suma ponderada de los vectores de entrada.
   - Se representa usando una matriz `A` con valores que suman 1 en cada fila.

3. **Self-Attention**
   - Calcula la atenci√≥n basada en la similitud entre vectores de entrada.
   - Uso de `softmax(X @ X.T)` para obtener la matriz de atenci√≥n.
   - Es la base de mecanismos de atenci√≥n en modelos Transformers.

4. **Visualizaci√≥n**
   - Flechas (`ax.arrow`) representan vectores originales y vectores de salida.
   - Colores distintos muestran qu√© vectores est√°n siendo atendidos.

üß© Requisitos

Antes de ejecutar el script, instala las dependencias:

pip install -r requirements.txt

üßë‚Äçüíª Autor

Desarrollado por Gus como parte de su aprendizaje en Python e IA.
